{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将人工标注的数据转化成crf模型格式\n",
    "def get_crf_data(line, words):\n",
    "    indexs = []\n",
    "    #for m in re.finditer(r'\\[(.*?)\\]',string):\n",
    "    #    indexs += [m.start(0),m.end(0)]\n",
    "    pre,flag,res = '',False,[]\n",
    "    for i in range(len(line)):\n",
    "        word = line[i]\n",
    "        if word != '[' or word != ']':\n",
    "            words.add(word)\n",
    "        if word == '[':\n",
    "            pre = word\n",
    "            continue\n",
    "        elif pre == '[':\n",
    "            res.append([word,'B'])\n",
    "            flag = True\n",
    "        elif word == ']':\n",
    "            res[-1][-1] = 'E'\n",
    "            flag = False\n",
    "        elif flag:\n",
    "            res.append([word, 'M'])\n",
    "        else:\n",
    "            res.append([word, 'O'])\n",
    "        pre = word\n",
    "    return res\n",
    "\n",
    "def process_file(path):\n",
    "    res, max_len, words = [], 0, set()\n",
    "    with codecs.open(path) as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            count = 0\n",
    "            for word, label in get_crf_data(line, words):\n",
    "                res.append(word + ' ' + label + '\\n')\n",
    "                count += 1\n",
    "            if count > max_len: max_len = count\n",
    "            res.append('\\n')\n",
    "    return max_len, words, res\n",
    "\n",
    "def write_data(lines,out_path):\n",
    "    with open(out_path,'w') as fout:\n",
    "        for line in lines:\n",
    "            fout.write(line)\n",
    "def write_words(words, out_path):\n",
    "    with codecs.open(out_path, 'w', encoding='gbk') as fout:\n",
    "        for word in words:\n",
    "            fout.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "max_len, words, res = process_file('data/bendibao.txt')\n",
    "write_data(res, 'data/bendibao.crf')\n",
    "print(max_len)\n",
    "write_words(words, 'data/words.txt')\n",
    "#print(word2id)\n",
    "#print(id2word)\n",
    "#get_crf_data('杭州[去哪吃夜宵]？[杭州吃夜宵好去处]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'abc[bde]ioe[oeds]in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bde', 'oeds']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\[(.*?)\\]',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = re.finditer(r'\\[(.*?)\\]',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bde] 3 8\n",
      "[oeds] 11 17\n"
     ]
    }
   ],
   "source": [
    "for m in res:\n",
    "    print(m.group(0),m.start(0),m.end(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools，工具类\n",
    "#将CRf数据格式进行处理，完成数据的填充，以及batch_size的数据集获取\n",
    "class Tools:\n",
    "    def __init__(self, seq_length):\n",
    "        self.pad_word = '<PAD>'\n",
    "        self.pad_tag = 'O'\n",
    "        self.seq_length = seq_length\n",
    "        self.words = []\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.tag2id = {}\n",
    "        \n",
    "        self.get_tag2id()\n",
    "        self.load_words('data/words.txt')\n",
    "        self.get_word_dict()\n",
    "    \n",
    "    def get_tag2id(self):\n",
    "        self.tag2id = {'O':[1.,.0,.0,.0], 'B':[.0,1.,.0,.0], 'M':[.0,.0,1.,.0], 'E':[.0,.0,.0,1.]}\n",
    "        \n",
    "    def load_words(self, path):\n",
    "        \n",
    "        with codecs.open(path, 'r', encoding='gbk') as fin:\n",
    "            for line in fin:\n",
    "                word = line.strip()\n",
    "                if len(word) == 0: continue\n",
    "                self.words.append(word)\n",
    "                \n",
    "    def get_word_dict(self):\n",
    "        for index, word in enumerate(self.words, 1):\n",
    "            self.word2id[word] = index\n",
    "            self.id2word[index] = word\n",
    "        self.word2id[self.pad_word] = 0\n",
    "        self.id2word[0] = self.pad_word\n",
    "    \n",
    "    def padding_sequence(self, sequence, seq_length):\n",
    "        train_list, tag_list = [], []\n",
    "        #print(sequence)\n",
    "        #print(seq_length)\n",
    "        for i in range(seq_length):\n",
    "            if i >= len(sequence):\n",
    "                train_list.append(self.word2id[self.pad_word])\n",
    "                tag_list.append(self.tag2id[self.pad_tag])\n",
    "            else:\n",
    "                try:\n",
    "                    train_list.append(self.word2id[sequence[i][0]])\n",
    "                    tag_list.append(self.tag2id[sequence[i][1]])\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    print(sequence)\n",
    "        return train_list, tag_list\n",
    "    \n",
    "    def process_file(self, path):\n",
    "        X, Y, sequence = [], [], []\n",
    "        with codecs.open(path, 'r', encoding='gbk') as fin:\n",
    "            for line in fin:\n",
    "                if line == '\\n':\n",
    "                    sub_x, sub_y = self.padding_sequence(sequence, self.seq_length)\n",
    "                    X.append(sub_x)\n",
    "                    Y.append(sub_y)\n",
    "                    sequence = []\n",
    "                else:\n",
    "                    items = line.strip().split(' ')\n",
    "                    if len(items) > 1:\n",
    "                        sequence.append(items)\n",
    "            return np.array(X), np.array(Y)\n",
    "    \n",
    "    def next_batch(self,X, Y, batch_size=64):\n",
    "        length = len(X)\n",
    "        num_batch = length // batch_size\n",
    "        indexs = list(range(length))\n",
    "        random.shuffle(indexs)\n",
    "        x_shuffle = X[indexs]\n",
    "        y_shuffle = Y[indexs]\n",
    "        \n",
    "        for i in range(num_batch):\n",
    "            start = i * batch_size\n",
    "            end = min((i+1) * batch_size, length)\n",
    "            yield x_shuffle[start:end], y_shuffle[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1745\n"
     ]
    }
   ],
   "source": [
    "tools = Tools(37)\n",
    "X,Y = tools.process_file('data/bendibao.train')\n",
    "X_val, Y_val = tools.process_file('data/bendibao.test')\n",
    "print(len(tools.word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4667 1444\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建lstm 网络\n",
    "class Lstm_model(object):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.seq_length = 37\n",
    "        self.num_classes = 4\n",
    "        self.lr = 0.02\n",
    "        self.lr_decay = 0.9\n",
    "        self.dropout_keep_pro = 0.8\n",
    "        self.num_epoch = 20\n",
    "        self.hidden_dim = 128\n",
    "        self.num_layer = 2\n",
    "        self.embedding_dim = 64\n",
    "        self.vocab_size = 1745\n",
    "        \n",
    "        self.run()\n",
    "        \n",
    "    def run(self):\n",
    "        #inpux\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(dtype=tf.float32, shape=[None, self.seq_length, self.num_classes], name='input_y')\n",
    "        self.dropout_keep_pro = tf.placeholder(dtype=tf.float32, name='keep_pro')\n",
    "        \n",
    "        #input_y 变形\n",
    "        self.labels = tf.reshape(self.input_y, [-1, self.num_classes])\n",
    "        \n",
    "        #lstm\n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(self.hidden_dim, state_is_tuple=True)\n",
    "        \n",
    "        def dropout():\n",
    "            cell = lstm_cell()\n",
    "            return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.dropout_keep_pro)\n",
    "        \n",
    "        def get_weight(shape):\n",
    "            return tf.Variable(tf.random_normal(shape=shape, stddev=0.1))\n",
    "        \n",
    "        #embedding\n",
    "        with tf.name_scope('embedding'):\n",
    "            embedding = tf.get_variable('embedding', shape=[self.vocab_size, self.embedding_dim], dtype=tf.float32)\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "        \n",
    "        with tf.name_scope('lstm'):\n",
    "            cells = [dropout() for i in range(self.num_layer)]\n",
    "            rnn_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "            outputs,_ = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=embedding_inputs, dtype=tf.float32)\n",
    "            print(embedding_inputs.shape)\n",
    "            print(outputs.shape)\n",
    "            print(outputs[:,-1,:].shape)\n",
    "            \n",
    "            #改变形状\n",
    "            outputs = tf.reshape(outputs, [-1, self.hidden_dim])\n",
    "        \n",
    "        with tf.name_scope('full_layer'):\n",
    "            weight1 = get_weight(shape=[self.hidden_dim, self.hidden_dim])\n",
    "            biases = tf.Variable(tf.constant(0.1,dtype=tf.float32, shape = [self.hidden_dim]))\n",
    "            fc1 = tf.matmul(outputs, weight1) + biases\n",
    "            fc1 = tf.nn.dropout(fc1, keep_prob=self.dropout_keep_pro)\n",
    "            fc1 = tf.nn.relu(fc1)\n",
    "            \n",
    "            weigth2 = get_weight(shape=[self.hidden_dim, self.num_classes])\n",
    "            biases2 = tf.Variable(tf.constant(0.1,dtype=tf.float32, shape = [self.num_classes]))\n",
    "            self.logits = tf.matmul(fc1, weigth2) + biases2\n",
    "            \n",
    "        with tf.name_scope('train_step'):\n",
    "            print('labels',self.labels.shape)\n",
    "            print('logits',self.logits.shape)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=self.logits)\n",
    "            #tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            loss = tf.reduce_mean(cross_entropy)\n",
    "            self.train_step = tf.train.GradientDescentOptimizer(self.lr).minimize(loss)\n",
    "        \n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.y_true = tf.arg_max(self.labels, 1)\n",
    "            self.y_pre = tf.arg_max(self.logits, 1)\n",
    "            correct_pre = tf.equal(self.y_pre, self.y_true)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_pre,dtype=tf.float32))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main(object):\n",
    "    def __init__(self, X, Y, X_val, Y_val):\n",
    "        self.model = Lstm_model()\n",
    "        self.save_path = 'model/lstm_model/model.ckpt'\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        \n",
    "    def evaluate(self, sess, x, y):\n",
    "        y_true = sess.run(self.model.y_true, feed_dict=\\\n",
    "                          {self.model.input_x:x, self.model.input_y:y, self.model.dropout_keep_pro:1.0})\n",
    "        y_pre = sess.run(self.model.y_pre, feed_dict=\\\n",
    "                         {self.model.input_x:x, self.model.input_y:y, self.model.dropout_keep_pro:1.0})\n",
    "        score = f1_score(y_true, y_pre, [0.,1.,2.,3.], average=None)\n",
    "        return 'c0 f1_score:%.3f, c1 f1_score:%.3f, c2 f1_score:%.3f, c3 f1_score:%0.3f' % tuple(score)\n",
    "\n",
    "    #训练模型\n",
    "    def train(self):\n",
    "        saver = tf.train.Saver()\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            print(X.shape)\n",
    "            print(Y.shape)\n",
    "            for epoch in range(300):\n",
    "                for xs,ys in tools.next_batch(self.X, self.Y):\n",
    "                    #print(xs.shape)\n",
    "                    #print(ys)\n",
    "                    sess.run(self.model.train_step, feed_dict = \\\n",
    "                             {self.model.input_x:xs, self.model.input_y:ys, self.model.dropout_keep_pro:0.8})\n",
    "                    #labels = sess.run(model.labels,feed_dict = {model.input_x:xs, model.input_y:ys, model.dropout_keep_pro:0.8})\n",
    "                    #logits = sess.run(model.logits,feed_dict = {model.input_x:xs, model.input_y:ys, model.dropout_keep_pro:0.8})\n",
    "                    #acc = sess.run(model.accuracy, feed_dict = {model.input_x:xs, model.input_y:ys, model.dropout_keep_pro:0.8})\n",
    "\n",
    "                    #print('Iter: ' + str(epoch) + ', Training Accuracy:' + str(acc))\n",
    "                self.model.lr *= self.model.lr_decay\n",
    "                train_acc = sess.run(self.model.accuracy, feed_dict = \\\n",
    "                    {self.model.input_x:self.X, self.model.input_y:self.Y, self.model.dropout_keep_pro:1.0})\n",
    "                test_acc = sess.run(self.model.accuracy, feed_dict = \\\n",
    "                    {self.model.input_x:self.X_val, self.model.input_y:self.Y_val, self.model.dropout_keep_pro:1.0})\n",
    "                f1_score_str = self.evaluate(sess, self.X_val, self.Y_val)\n",
    "                print('Iter: ' + str(epoch) + ', Training Accuracy:' + str(train_acc) + ', Testing Accuracy:'\\\n",
    "                      + str(test_acc) + ' | ' + f1_score_str)\n",
    "            saver.save(sess=sess, save_path=self.save_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main = Main(X, Y, X_val, Y_val)\n",
    "main.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntf.reset_default_graph()\\nx = tf.placeholder(dtype=tf.int32, shape=[None, 37])\\nembedding = tf.get_variable('embedding', shape=[1745, 64], dtype=tf.float32)\\nembedding_inputs = tf.nn.embedding_lookup(embedding, x)\\ninit = tf.global_variables_initializer()\\nwith tf.Session() as sess:\\n    sess.run(init)\\n    for xs,ys in tools.next_batch(X, Y):\\n        print(xs, xs.shape)\\n        res_em = sess.run(embedding_inputs, feed_dict = {x:xs})\\n        print(res_em)\\n        print(res_em.shape)\\n        break\\n\""
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 测试embedding_lookup\n",
    "'''\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(dtype=tf.int32, shape=[None, 37])\n",
    "embedding = tf.get_variable('embedding', shape=[1745, 64], dtype=tf.float32)\n",
    "embedding_inputs = tf.nn.embedding_lookup(embedding, x)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for xs,ys in tools.next_batch(X, Y):\n",
    "        print(xs, xs.shape)\n",
    "        res_em = sess.run(embedding_inputs, feed_dict = {x:xs})\n",
    "        print(res_em)\n",
    "        print(res_em.shape)\n",
    "        break\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
